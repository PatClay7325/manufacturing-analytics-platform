# Ollama Optimized Configuration for Low-Resource Systems

# Ollama Connection
OLLAMA_API_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=gemma:2b

# Enable Streaming (Recommended for better UX)
OLLAMA_USE_STREAMING=true

# Performance Optimization
OLLAMA_MAX_CONTEXT=2048        # Reduced context window (default: 4096)
OLLAMA_ENABLE_CACHE=true       # Cache responses to reduce load
OLLAMA_CACHE_TTL=300          # Cache for 5 minutes
OLLAMA_TIMEOUT=60000          # 60 second timeout
OLLAMA_NUM_THREADS=2          # Limit CPU threads (adjust based on your CPU)
OLLAMA_NUM_GPU=0              # Disable GPU (set to 1 if you have GPU)

# Response Settings
OLLAMA_MAX_TOKENS=500         # Limit response length
OLLAMA_TEMPERATURE=0.7        # Balance creativity/accuracy
OLLAMA_TOP_P=0.9             # Nucleus sampling
OLLAMA_TOP_K=40              # Limit vocabulary

# Memory Management
OLLAMA_BATCH_SIZE=256         # Smaller batch size
OLLAMA_EMBEDDING_BATCH_SIZE=5 # Process embeddings in small batches

# Additional Performance Flags
NODE_OPTIONS="--max-old-space-size=2048"  # Limit Node.js memory